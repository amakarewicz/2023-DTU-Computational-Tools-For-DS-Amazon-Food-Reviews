{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from signatures_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_feather('data/text_df_1.feather'),\n",
    "                  pd.read_feather('data/text_df_2.feather'),\n",
    "                  pd.read_feather('data/text_df_3.feather')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56845, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = data.sample(int(len(data)/10), random_state=123)\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_clean = re.compile(r'[\\d.,!?;:@#&]+|<[^>]*>|https?://\\S+|[\"\\'\\(\\)_/+\\\\$-]')\n",
    "data_sample['Cleaner_text'] = (\n",
    "    data_sample['Text']\n",
    "    .apply(lambda x: html_clean.sub(r' ',x))\n",
    "    .apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    ")\n",
    "\n",
    "data_sample['Ngrams'] = (\n",
    "    data_sample['Cleaner_text']\n",
    "    .apply(lambda x: set(list(nltk.ngrams(x.lower().split(), 2))))\n",
    "    .apply(lambda x: [\" \".join(w) for w in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = data_sample[['UserId', 'Ngrams']].groupby('UserId').apply('sum')\n",
    "data_agg['Ngrams'] = data_agg['Ngrams'].apply(lambda x: set(x))\n",
    "\n",
    "k=100\n",
    "data_agg['Minhash'] = data_agg['Ngrams'].apply(lambda x: minhash2(x, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agg = data_agg.reset_index()\n",
    "data_agg.to_csv('data/sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2791/2791 [04:19<00:00, 10.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393\n",
      "3896236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "seed = 123\n",
    "jaccard_threshold = 0.035\n",
    "\n",
    "lsh_dict = dict(zip(data_agg.index, data_agg.Minhash))\n",
    "list_keys = list(lsh_dict.keys())\n",
    "\n",
    "similar_items = {}\n",
    "count = 0\n",
    "for i in tqdm(range(len(list_keys)-1)):\n",
    "    for j in range(i+1, len(list_keys)):\n",
    "        count +=1\n",
    "        common_values = np.intersect1d(lsh_dict[list_keys[i]], lsh_dict[list_keys[j]])\n",
    "        if len(common_values) > 0:\n",
    "            # we found a candidate\n",
    "            similarity_score = jaccard(list_keys[i], list_keys[j], lsh_dict)\n",
    "            if similarity_score > jaccard_threshold:\n",
    "                # print(similarity_score)\n",
    "                similar_items[(list_keys[i], list_keys[j])] = similarity_score\n",
    "print(len(similar_items))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_items_new = {data_agg.UserId[k[0]]+\"|\"+data_agg.UserId[k[1]]:v for k, v in similar_items.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json object from dictionary\n",
    "import json\n",
    "json = json.dumps(similar_items_new)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"data/similarity_dict.json\",\"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(json)\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
