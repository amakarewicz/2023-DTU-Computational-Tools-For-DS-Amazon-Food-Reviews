{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# importing functions for signatures calculation\n",
    "from signatures_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28422, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "data = pd.concat([pd.read_feather('data/text_df_1.feather'),\n",
    "                  pd.read_feather('data/text_df_2.feather'),\n",
    "                  pd.read_feather('data/text_df_3.feather')])\n",
    "# creating a sample\n",
    "data_sample = data.sample(int(len(data)/20), random_state=123)\n",
    "data_sample.to_csv('data/sample_no_agg.csv', index=False)\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the reviews from punctuations, html tags etc.\n",
    "html_clean = re.compile(r'[\\d.,!?;:@#&]+|<[^>]*>|https?://\\S+|[\"\\'\\(\\)_/+\\\\$-]')\n",
    "data_sample['Cleaner_text'] = (\n",
    "    data_sample['Text']\n",
    "    .apply(lambda x: html_clean.sub(r' ',x))\n",
    "    .apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    ")\n",
    "# extracting 2-grams (shingles)\n",
    "data_sample['Ngrams'] = (\n",
    "    data_sample['Cleaner_text']\n",
    "    .apply(lambda x: set(list(nltk.ngrams(x.lower().split(), 2))))\n",
    "    .apply(lambda x: [\" \".join(w) for w in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minhashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating 2-grams per user\n",
    "data_agg = data_sample[['UserId', 'Ngrams']].groupby('UserId').apply('sum')\n",
    "# dropping duplicated 2-grams\n",
    "data_agg['Ngrams'] = data_agg['Ngrams'].apply(lambda x: set(x))\n",
    "# minhashing\n",
    "k = 100\n",
    "data_agg['Minhash'] = data_agg['Ngrams'].apply(lambda x: minhash2(x, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving aggregated data\n",
    "data_agg = data_agg.reset_index()\n",
    "data_agg.to_csv('data/sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24556/24556 [2:10:29<00:00,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668550\n",
      "301510846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "# threshold for similarity between users\n",
    "jaccard_threshold = 0.035\n",
    "# dictionary with signatures ('user' : signature)\n",
    "lsh_dict = dict(zip(data_agg.index, data_agg.Minhash))\n",
    "list_keys = list(lsh_dict.keys())\n",
    "# loop for calculating the similarity\n",
    "similar_items = {}\n",
    "count = 0\n",
    "# iterate over users\n",
    "for i in tqdm(range(len(list_keys)-1)):\n",
    "    for j in range(i+1, len(list_keys)):\n",
    "        count +=1\n",
    "        # find common values within signatures of two users\n",
    "        common_values = np.intersect1d(lsh_dict[list_keys[i]], lsh_dict[list_keys[j]])\n",
    "        # if there are any common values\n",
    "        if len(common_values) > 0:\n",
    "            # calculate the similarity\n",
    "            similarity_score = jaccard(list_keys[i], list_keys[j], lsh_dict)\n",
    "            # if similarity is higher than threshold\n",
    "            if similarity_score > jaccard_threshold:\n",
    "                # saving pair of users to dict alongside their similarity score\n",
    "                similar_items[(list_keys[i], list_keys[j])] = similarity_score\n",
    "# number of pair of users with similarity higher than threshold\n",
    "print(len(similar_items))\n",
    "# all possible pairs of users\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving similarity information into a dictionary ('user1|user2' : similarity)\n",
    "similar_items_new = {data_agg.UserId[k[0]]+\"|\"+data_agg.UserId[k[1]]:v for k, v in similar_items.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json object from dictionary\n",
    "json = json.dumps(similar_items_new)\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"data/similarity_dict.json\",\"w\")\n",
    "# write json object to file\n",
    "f.write(json)\n",
    "# close file\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
